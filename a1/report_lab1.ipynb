{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 Report\n",
    "## Problem 1: Linear Regression\n",
    "### 1.1 Loss function and gradient\n",
    "\n",
    "$$L = L_{d} + L_{W} = \\sum_{n=1}^N\\frac{1}{2N} \\|W^{T}x^{(n)} + b - y^{(n)}\\|^2 + \\frac{\\lambda}{2}\\|W\\|^2$$\n",
    "\n",
    "The gradient with respect to b is \n",
    "$$\\frac{\\partial L}{\\partial b} = \\sum_{n=1}^N\\frac{1}{2N}(2b+2W^{T}x^{(n)} - 2y_{n})$$\n",
    "\n",
    "The gradient with respect to W is\n",
    "$$\\frac{\\partial L}{\\partial W} = \\sum_{n=1}^N\\frac{1}{2N}(x^{(n)^{T}}Wx^{(n)} + 2bx^{(n)} - 2y_{n}x^{(n)}) + {\\lambda}W$$\n",
    "\n",
    "Following is the code snippet of MSE and gradMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(W, b, x, y, reg):\n",
    "    N, M= x.shape   # 3500x784\n",
    "    mse = la.norm(np.dot(x, W) + b - y) ** 2\n",
    "    total_loss = 1 / (2 * N) * mse + reg / 2 * (la.norm(W) ** 2)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def gradMSE(W, b, x, y, reg):\n",
    "    N, M= x.shape   # 3500x784\n",
    "    grad_term = np.dot(x, W) + b - y\n",
    "    gradMSE_bias = 1/N * np.sum(grad_term)\n",
    "    gradMSE_W = 1/N * np.dot(x.T, grad_term) + reg * W\n",
    "    \n",
    "    return gradMSE_bias, gradMSE_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Gradient Descent Implementation\n",
    "Following is the code snippet of grad_descent for MSE only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_descent(W, b, trainingData, trainingLabels, alpha, epochs, reg, EPS):\n",
    "    W_comb = []\n",
    "    b_comb = []\n",
    "\n",
    "    if lossType == \"None\":\n",
    "        print('in GD with \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg))\n",
    "        train_W = W\n",
    "        train_bias = b\n",
    "        losses = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            gradMSE_bias, gradMSE_W = gradMSE(train_W, train_bias, trainingData, trainingLabels, reg)\n",
    "            old_W = train_W\n",
    "            train_W = train_W - alpha * gradMSE_W\n",
    "            if la.norm(train_W - old_W) < EPS:\n",
    "                break;\n",
    "            train_bias = train_bias - alpha * gradMSE_bias\n",
    "            mse = MSE(train_W, train_bias, trainingData, trainingLabels, reg)\n",
    "            W_comb.append(train_W)\n",
    "            b_comb.append(train_bias)\n",
    "            losses.append(mse)\n",
    "        print('GD with \\u03B1 = {}, \\u03BB = {} finished'.format(alpha, reg))\n",
    "    return b_comb, W_comb, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tuning the Learning Rate\n",
    "We train the data with $\\alpha$ = {0.005, 0.001, 0.0001} and plotted the training, validation and test losses on each $\\alpha$. Following are the figures.\n",
    "![alt text](train_loss_linear.png)\n",
    "<center>Figure 1: losses on training data with $\\alpha$ = 0.005, 0.001, 0.0001 respecively</center>\n",
    "![title](valid_loss_linear.png)\n",
    "<center>Figure 2: losses on validation data with $\\alpha$ = 0.005, 0.001, 0.0001 respecively</center>\n",
    "![title](test_loss_linear.png)\n",
    "<center>Figure 3: losses on test data with $\\alpha$ = 0.005, 0.001, 0.0001 respecively</center>\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "<center>Table 1: Accuracy of all sets of data with $\\alpha$ = 0.005, 0.001, 0.0001 respecively</center>\n",
    "<style>\n",
    "td {\n",
    "  font-size: 100px\n",
    "}\n",
    "</style>\n",
    "\n",
    "| | Training | Validation | Test |\n",
    "| --- | --- | --- |\n",
    "| $\\alpha$ = 0.005 | 0.758 | 0.67 | 0.744 |\n",
    "| $\\alpha$ = 0.001 | 0.649 | 0.61 | 0.572 |\n",
    "| $\\alpha$ = 0.0001 | 0.554 | 0.57 | 0.544 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Generalization\n",
    "We train the data with fixed $\\alpha$ = 0.005, $\\lambda$ = {0.001, 0.1, 0.5}. We plotted the training, validation and test losses on each $\\lambda$. Following are the figures.\n",
    "![alt text](train_loss_linear_reg.png)\n",
    "<center>Figure 4: losses on training data with $\\lambda$ = 0.001, 0.1, 0.5 respecively</center>\n",
    "![title](valid_loss_linear_reg.png)\n",
    "<center>Figure 5: losses on validation data with $\\lambda$ = 0.001, 0.1, 0.5 respecively</center>\n",
    "![title](test_loss_linear_reg.png)\n",
    "<center>Figure 6: losses on test data with $\\lambda$ = 0.001, 0.1, 0.5 respecively</center>\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "<center>Table 2: Accuracy of all sets of data with $\\lambda$ = 0.001, 0.1, 0.5 respecively</center>\n",
    "<style>\n",
    "td {\n",
    "  font-size: 100px\n",
    "}\n",
    "</style>\n",
    "\n",
    "| | Training | Validation | Test |\n",
    "| --- | --- | --- |\n",
    "| $\\lambda$ = 0.001 | 0.763 | 0.68 | 0.751 |\n",
    "| $\\lambda$ = 0.1 | 0.977 | 0.98 | 0.965 |\n",
    "| $\\lambda$ = 0.5 | 0.976 | 0.97 | 0.965 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Comparing Batch GD with normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 2: Logistic Regression\n",
    "### 2.1 Binary cross-entropy loss\n",
    "\n",
    "$$L=L_{d}+L_{W} = \\sum_{n=1}^N\\frac{1}{N} [-y_{n}log\\hat{y}(x^{(n)})-(1-y_{n})log(1-\\hat{y}(x^{(n)}))] + \\frac{\\lambda}{2}\\|W\\|^2$$\n",
    "\n",
    "The gradient with respect to b is \n",
    "$$\\frac{\\partial L}{\\partial b} = -\\frac{1}{N}\\sum_{n=1}^N[y_{n} - \\frac{1}{1+e^{-(W^{T}x^{(n)} + b)}}]$$\n",
    "\n",
    "The gradient with respect to W is\n",
    "$$\\frac{\\partial L}{\\partial W} = -\\frac{1}{N}\\sum_{n=1}^N [y_{n} - \\frac{1}{1+e^{-(W^{T}x^{(n)} + b)}}]x^{(n)} + {\\lambda}W$$\n",
    "\n",
    "Following is the code snippet of crossEntropyLoss and gradCE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossEntropyLoss(W, b, x, y, reg):\n",
    "    N, M = x.shape   # 3500x784\n",
    "    sigmoid = 1/(1 + np.exp(-np.dot(x, W) - b))  #sigmoid function with input Wx+b\n",
    "    cross_entropy = np.multiply(y, np.log(sigmoid)) + np.multiply(1-y, np.log(1 - sigmoid))\n",
    "    total_loss = -1/N * np.sum(cross_entropy) + reg / 2 * (la.norm(W) ** 2)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def gradCE(W, b, x, y, reg):\n",
    "    N, M = x.shape  # 3500x784\n",
    "    sigmoid = 1 / (1 + np.exp(-np.dot(x, W) - b))\n",
    "    gradCE_bias = -1/N * np.sum(y - sigmoid)\n",
    "    gradCE_W = -1/N * np.dot(x.T, y - sigmoid) + reg * W\n",
    "    return gradCE_bias, gradCE_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Learning\n",
    "We train the data with fixed $\\alpha$ = 0.005, $\\lambda$ = 0.1. We plotted the training, validation and test losses and accuracy. Following is the code snippet of modified grad_descent supporting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_descent(W, b, trainingData, trainingLabels, alpha, epochs, reg, EPS, lossType=\"None\"):\n",
    "    W_comb = []\n",
    "    b_comb = []\n",
    "\n",
    "    if lossType == \"None\":\n",
    "        print('in GD with \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg))\n",
    "        train_W = W\n",
    "        train_bias = b\n",
    "        losses = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            gradMSE_bias, gradMSE_W = gradMSE(train_W, train_bias, trainingData, trainingLabels, reg)\n",
    "            old_W = train_W\n",
    "            train_W = train_W - alpha * gradMSE_W\n",
    "            if la.norm(train_W - old_W) < EPS:\n",
    "                break;\n",
    "            train_bias = train_bias - alpha * gradMSE_bias\n",
    "            mse = MSE(train_W, train_bias, trainingData, trainingLabels, reg)\n",
    "            W_comb.append(train_W)\n",
    "            b_comb.append(train_bias)\n",
    "            losses.append(mse)\n",
    "        # plt.plot(losses, label='MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg))\n",
    "        print('GD with \\u03B1 = {}, \\u03BB = {} finished'.format(alpha, reg))\n",
    "    else:\n",
    "        b_comb, W_comb, losses = grad_descent_CE(W, b, trainingData, trainingLabels, alpha, epochs, reg, EPS)\n",
    "\n",
    "    return b_comb, W_comb, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following figure shows the losses and accuracy on training, validation, and test data with $\\alpha$ = 0.005, $\\lambda$ = 0.1.\n",
    "\n",
    "![alt text](log_loss_acc.png)\n",
    "<center>Figure 7: losses and accuracy on all data with $\\alpha$ = 0.005, $\\lambda$ = 0.1</center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Comparison to Linear Regression\n",
    "Following figure shows comparison between cross entropy and mse losses.\n",
    "![alt text](linear_log_compare.png)\n",
    "<center>Figure 8: cross entropy and MSE loss comparison ($\\alpha$ = 0.005, $\\lambda$ = 0)</center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 3: Batch Gradient Descent vs. SGD and Adam\n",
    "### 3.1 SGD\n",
    "Following is the code snippet of buildGraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildGraph(beta1, beta2, epsilon, loss=\"None\"):\n",
    "    W = tf.Variable(tf.truncated_normal([784, 1], stddev=0.5, dtype=tf.float32))\n",
    "    b = tf.Variable(tf.zeros(1))\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y = tf.placeholder(tf.float32, [None, 1])\n",
    "    lambda_ = tf.placeholder(tf.float32)\n",
    "    tf.set_random_seed(421)\n",
    "\n",
    "    if loss == \"MSE\":\n",
    "        y_hat = tf.matmul(x, W) + b\n",
    "        loss_t = 0.5 * tf.reduce_mean(tf.square(y - y_hat)) + lambda_ * tf.nn.l2_loss(W)\n",
    "    elif loss == \"CE\":\n",
    "        logits = (tf.matmul(x, W) + b)\n",
    "        loss_t = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits) + lambda_ * tf.nn.l2_loss(W)\n",
    "\n",
    "    adam_op = tf.train.AdamOptimizer(learning_rate=0.001, beta1=beta1, beta2=beta2, epsilon=epsilon).minimize(loss_t)\n",
    "    return x, y, W, b, lambda_, loss_t, adam_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implementing Stochastic Gradient Descent\n",
    "Following is the code snippet of SGD implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatch(minibatch_size, trainingData, trainingTarget, beta1, beta2, epsilon, plot_loss, lossType):\n",
    "    N, _ = trainingData.shape\n",
    "    x, y, W, b, lambda_, loss_t, adam_op = buildGraph(beta1, beta2, epsilon, loss=lossType)\n",
    "\n",
    "    n_epochs = 700\n",
    "    iterations = N // minibatch_size\n",
    "\n",
    "    # \"minibatch\" training\n",
    "    acc_train = np.zeros(n_epochs)\n",
    "    loss_train = np.zeros(n_epochs)\n",
    "    acc_valid = np.zeros(n_epochs)\n",
    "    loss_valid = np.zeros(n_epochs)\n",
    "    acc_test = np.zeros(n_epochs)\n",
    "    loss_test = np.zeros(n_epochs)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        #shuffle\n",
    "        s = np.arange(N)\n",
    "        np.random.shuffle(s)\n",
    "        trainingData = trainingData[s]\n",
    "        trainingTarget = trainingTarget[s]\n",
    "\n",
    "        #iterating mini batch\n",
    "        for j in range(iterations):\n",
    "            batch_data = trainingData[j*minibatch_size:(j+1)*minibatch_size, :]\n",
    "            batch_target = trainingTarget[j*minibatch_size:(j+1)*minibatch_size, :]\n",
    "            _, train_W, train_b = sess.run([adam_op, W, b], feed_dict={x: batch_data, y: batch_target, lambda_: 0})\n",
    "\n",
    "        #calc accuracy and loss for each epoch\n",
    "        acc_train[i] = np.sum((np.dot(trainingData, train_W) + train_b >= 0.5) == trainingTarget) / trainingTarget.shape[0]\n",
    "        loss_train[i] = sess.run(loss_t, feed_dict={x: trainingData, y: trainingTarget, lambda_: 0})\n",
    "        #valid and test\n",
    "        acc_valid[i] = np.sum((np.dot(validData, train_W) + train_b >= 0.5) == validTarget) / validTarget.shape[0]\n",
    "        loss_valid[i] = sess.run(loss_t, feed_dict={x: validData, y: validTarget, lambda_: 0})\n",
    "        acc_test[i] = np.sum((np.dot(testData, train_W) + train_b >= 0.5) == testTarget) / testTarget.shape[0]\n",
    "        loss_test[i] = sess.run(loss_t, feed_dict={x: testData, y: testTarget, lambda_: 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follosing figure shows a general case of minibatch size 500.\n",
    "![alt text](SGD_MSE_500.png)\n",
    "<center>Figure 9: SGD with a minibatch size of 500 (MSE, $\\lambda$ = 0)</center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Batch Size Investigation\n",
    "Following figures show the effects on minibatch size changeing.\n",
    "![alt text](SGD_MSE_100.png)\n",
    "<center>Figure 10: SGD with a minibatch size of 100 (MSE, $\\lambda$ = 0)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_MSE_700.png)\n",
    "<center>Figure 11: SGD with a minibatch size of 700 (MSE, $\\lambda$ = 0)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_MSE_1750.png)\n",
    "<center>Figure 12: SGD with a minibatch size of 1750 (MSE, $\\lambda$ = 0)</center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Hyperparameter Investigation\n",
    "Following figures show the effects on hyperparmeters.\n",
    "![alt text](SGD_MSE_BETA1.png)\n",
    "<center>Figure 13: SGD with $\\beta_{1}$ = 0.95, 0.99 (MSE, minibatch size 500)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_MSE_BETA2.png)\n",
    "<center>Figure 14: SGD with $\\beta_{2}$ = 0.99, 0.9999 (MSE, minibatch size 500)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_MSE_EPI.png)\n",
    "<center>Figure 15: SGD with $\\epsilon$ = 1e-9, 1e-4 (MSE, minibatch size 500)</center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Cross Entropy Loss Investigation\n",
    "Following figures show the losses calculated using cross entropy. The general demonstration uses minibatch size of 500. And the other hyperparameters investigation can be compared with those in Section 3.4.\n",
    "![alt text](SGD_CE_500.png)\n",
    "<center>Figure 16: SGD with a minibatch size of 500 (CE, $\\lambda$ = 0)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_CE_BETA1.png)\n",
    "<center>Figure 17: SGD with $\\beta_{1}$ = 0.95, 0.99 (CE, minibatch size 500)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_CE_BETA2.png)\n",
    "<center>Figure 18: SGD with $\\beta_{2}$ = 0.99, 0.9999 (CE, minibatch size 500)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_CE_EPI.png)\n",
    "<center>Figure 19: SGD with $\\epsilon$ = 1e-9, 1e-4 (CE, minibatch size 500)</center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Comparison against Batch GD\n",
    "Comparison between CE batch gradient descent and SGD:\n",
    "![alt](log_loss_acc.png)\n",
    "<center>Figure 7: losses and accuracy on all data (CE)</center>\n",
    "![alt](SGD_CE_500.png)\n",
    "<center>Figure 16: SGD with a minibatch size of 500 (CE)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
