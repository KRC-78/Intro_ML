{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 Report\n",
    "## Problem 1: Linear Regression\n",
    "### 1.1 Loss function and gradient\n",
    "\n",
    "$$L = L_{d} + L_{W} = \\sum_{n=1}^N\\frac{1}{2N} \\|W^{T}x^{(n)} + b - y^{(n)}\\|^2 + \\frac{\\lambda}{2}\\|W\\|^2$$\n",
    "\n",
    "The gradient with respect to b is \n",
    "$$\\frac{\\partial L}{\\partial b} = \\sum_{n=1}^N\\frac{1}{2N}(2b+2W^{T}x^{(n)} - 2y_{n})$$\n",
    "\n",
    "The gradient with respect to W is\n",
    "$$\\frac{\\partial L}{\\partial W} = \\sum_{n=1}^N\\frac{1}{2N}(x^{(n)^{T}}Wx^{(n)} + 2bx^{(n)} - 2y_{n}x^{(n)}) + {\\lambda}W$$\n",
    "\n",
    "Following is the code snippet of MSE and gradMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(W, b, x, y, reg):\n",
    "    N, M= x.shape   # 3500x784\n",
    "    mse = la.norm(np.dot(x, W) + b - y) ** 2\n",
    "    total_loss = 1 / (2 * N) * mse + reg / 2 * (la.norm(W) ** 2)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def gradMSE(W, b, x, y, reg):\n",
    "    N, M= x.shape   # 3500x784\n",
    "    grad_term = np.dot(x, W) + b - y\n",
    "    gradMSE_bias = 1/N * np.sum(grad_term)\n",
    "    gradMSE_W = 1/N * np.dot(x.T, grad_term) + reg * W\n",
    "    \n",
    "    return gradMSE_bias, gradMSE_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Gradient Descent Implementation\n",
    "Following is the code snippet of grad_descent for MSE only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_descent(W, b, trainingData, trainingLabels, alpha, epochs, reg, EPS):\n",
    "    W_comb = []\n",
    "    b_comb = []\n",
    "\n",
    "    if lossType == \"None\":\n",
    "        print('in GD with \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg))\n",
    "        train_W = W\n",
    "        train_bias = b\n",
    "        losses = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            gradMSE_bias, gradMSE_W = gradMSE(train_W, train_bias, trainingData, trainingLabels, reg)\n",
    "            old_W = train_W\n",
    "            train_W = train_W - alpha * gradMSE_W\n",
    "            if la.norm(train_W - old_W) < EPS:\n",
    "                break;\n",
    "            train_bias = train_bias - alpha * gradMSE_bias\n",
    "            mse = MSE(train_W, train_bias, trainingData, trainingLabels, reg)\n",
    "            W_comb.append(train_W)\n",
    "            b_comb.append(train_bias)\n",
    "            losses.append(mse)\n",
    "        print('GD with \\u03B1 = {}, \\u03BB = {} finished'.format(alpha, reg))\n",
    "    return b_comb, W_comb, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tuning the Learning Rate\n",
    "We train the data with $\\alpha$ = {0.005, 0.001, 0.0001} and plotted the training, validation and test losses on each $\\alpha$. Following are the figures.\n",
    "![alt text](train_loss_linear.png)\n",
    "<center>Figure 1: losses on training data with $\\alpha$ = 0.005, 0.001, 0.0001 respecively</center>\n",
    "![title](valid_loss_linear.png)\n",
    "<center>Figure 2: losses on validation data with $\\alpha$ = 0.005, 0.001, 0.0001 respecively</center>\n",
    "![title](test_loss_linear.png)\n",
    "<center>Figure 3: losses on test data with $\\alpha$ = 0.005, 0.001, 0.0001 respecively</center>\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "<center>Table 1: Accuracy of all sets of data with $\\alpha$ = 0.005, 0.001, 0.0001 respecively</center>\n",
    "<style>\n",
    "td {\n",
    "  font-size: 100px\n",
    "}\n",
    "</style>\n",
    "\n",
    "| | Training | Validation | Test |\n",
    "| --- | --- | --- |\n",
    "| $\\alpha$ = 0.005 | 0.758 | 0.67 | 0.744 |\n",
    "| $\\alpha$ = 0.001 | 0.649 | 0.61 | 0.572 |\n",
    "| $\\alpha$ = 0.0001 | 0.554 | 0.57 | 0.544 |\n",
    "\n",
    "By modifying the $\\alpha$ with 5000 itertations, all three losses plot indicate the similiar tendency that the loss decrease gradually with more iterations. The plot with larger $\\alpha$ is more convex than the smaller ones, the loss decrease more rapidly with a larger $\\alpha$ than smaller ones. \n",
    "For the final accuracy result, the accuracy keeps decreasing as $\\alpha$ is decreasing with same number of iterations(in this case, 5000 iterations), because it takes more time to low the loss when the $\\alpha$ is small, and more iterations are needed to low the loss for a smaller $\\alpha$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Generalization\n",
    "We train the data with fixed $\\alpha$ = 0.005, $\\lambda$ = {0.001, 0.1, 0.5}. We plotted the training, validation and test losses on each $\\lambda$. Following are the figures.\n",
    "![alt text](train_loss_linear_reg.png)\n",
    "<center>Figure 4: losses on training data with $\\lambda$ = 0.001, 0.1, 0.5 respecively</center>\n",
    "![title](valid_loss_linear_reg.png)\n",
    "<center>Figure 5: losses on validation data with $\\lambda$ = 0.001, 0.1, 0.5 respecively</center>\n",
    "![title](test_loss_linear_reg.png)\n",
    "<center>Figure 6: losses on test data with $\\lambda$ = 0.001, 0.1, 0.5 respecively</center>\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "<center>Table 2: Accuracy of all sets of data with $\\lambda$ = 0.001, 0.1, 0.5 respecively</center>\n",
    "<style>\n",
    "td {\n",
    "  font-size: 100px\n",
    "}\n",
    "</style>\n",
    "\n",
    "| | Training | Validation | Test |\n",
    "| --- | --- | --- |\n",
    "| $\\lambda$ = 0.001 | 0.763 | 0.68 | 0.751 |\n",
    "| $\\lambda$ = 0.1 | 0.977 | 0.98 | 0.965 |\n",
    "| $\\lambda$ = 0.5 | 0.976 | 0.97 | 0.965 |",
    "\n",
    "The larger regularization paramer will result in higher initial value, however, the loss will decrease quickly until it approach zero. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Comparing Batch GD with normal equation\n",
    "We compared the result from normal equation and that from BGD with $\\alpha$ = 0.005 and $\\lambda$ = 0.001. Following table shows the results.\n",
    "<br><br>\n",
    "\n",
    "<center>Table 3: Comparison between normal equation and BGD on computation time, losses and accuracy</center>\n",
    "\n",
    "| | Computation time (s) | training MSE loss | accuracy on training data |accuracy on validation data|accuracy on test data|\n",
    "| --- | --- | --- |\n",
    "| Normal equation | 0.261 | 0.00935 | 0.993 | 0.96 | 0.944 |\n",
    "| Batch GD | 16.876 | 0.01365 | 0.978 | 0.98 | 0.972 |",
    "\n",
    "From the table above, the computation time of Normal equation is much less than Batch GD. Normal equiation also generates a lower training MSE loss than Batch GD. The accuracy of two different method is similiar, the Normal equation(accuracy = 0.993) is slightly more accurate than the Batch GD(accuracy = 0.978) in this case. However, if comparing the accruacy result using validation and test datasets, Batch GD has slightly higher performance due to higher accuracy value. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 2: Logistic Regression\n",
    "### 2.1 Binary cross-entropy loss\n",
    "\n",
    "$$L=L_{d}+L_{W} = \\sum_{n=1}^N\\frac{1}{N} [-y_{n}log\\hat{y}(x^{(n)})-(1-y_{n})log(1-\\hat{y}(x^{(n)}))] + \\frac{\\lambda}{2}\\|W\\|^2$$\n",
    "\n",
    "The gradient with respect to b is \n",
    "$$\\frac{\\partial L}{\\partial b} = -\\frac{1}{N}\\sum_{n=1}^N[y_{n} - \\frac{1}{1+e^{-(W^{T}x^{(n)} + b)}}]$$\n",
    "\n",
    "The gradient with respect to W is\n",
    "$$\\frac{\\partial L}{\\partial W} = -\\frac{1}{N}\\sum_{n=1}^N [y_{n} - \\frac{1}{1+e^{-(W^{T}x^{(n)} + b)}}]x^{(n)} + {\\lambda}W$$\n",
    "\n",
    "Following is the code snippet of crossEntropyLoss and gradCE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossEntropyLoss(W, b, x, y, reg):\n",
    "    N, M = x.shape   # 3500x784\n",
    "    sigmoid = 1/(1 + np.exp(-np.dot(x, W) - b))  #sigmoid function with input Wx+b\n",
    "    cross_entropy = np.multiply(y, np.log(sigmoid)) + np.multiply(1-y, np.log(1 - sigmoid))\n",
    "    total_loss = -1/N * np.sum(cross_entropy) + reg / 2 * (la.norm(W) ** 2)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def gradCE(W, b, x, y, reg):\n",
    "    N, M = x.shape  # 3500x784\n",
    "    sigmoid = 1 / (1 + np.exp(-np.dot(x, W) - b))\n",
    "    gradCE_bias = -1/N * np.sum(y - sigmoid)\n",
    "    gradCE_W = -1/N * np.dot(x.T, y - sigmoid) + reg * W\n",
    "    return gradCE_bias, gradCE_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Learning\n",
    "We train the data with fixed $\\alpha$ = 0.005, $\\lambda$ = 0.1. We plotted the training, validation and test losses and accuracy. Following is the code snippet of modified grad_descent supporting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_descent(W, b, trainingData, trainingLabels, alpha, epochs, reg, EPS, lossType=\"None\"):\n",
    "    W_comb = []\n",
    "    b_comb = []\n",
    "\n",
    "    if lossType == \"None\":\n",
    "        print('in GD with \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg))\n",
    "        train_W = W\n",
    "        train_bias = b\n",
    "        losses = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            gradMSE_bias, gradMSE_W = gradMSE(train_W, train_bias, trainingData, trainingLabels, reg)\n",
    "            old_W = train_W\n",
    "            train_W = train_W - alpha * gradMSE_W\n",
    "            if la.norm(train_W - old_W) < EPS:\n",
    "                break;\n",
    "            train_bias = train_bias - alpha * gradMSE_bias\n",
    "            mse = MSE(train_W, train_bias, trainingData, trainingLabels, reg)\n",
    "            W_comb.append(train_W)\n",
    "            b_comb.append(train_bias)\n",
    "            losses.append(mse)\n",
    "        # plt.plot(losses, label='MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg))\n",
    "        print('GD with \\u03B1 = {}, \\u03BB = {} finished'.format(alpha, reg))\n",
    "    else:\n",
    "        b_comb, W_comb, losses = grad_descent_CE(W, b, trainingData, trainingLabels, alpha, epochs, reg, EPS)\n",
    "\n",
    "    return b_comb, W_comb, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following figure shows the losses and accuracy on training, validation, and test data with $\\alpha$ = 0.005, $\\lambda$ = 0.1.\n",
    "\n",
    "![alt text](log_loss_acc.png)\n",
    "<center>Figure 7: losses and accuracy on all data with $\\alpha$ = 0.005, $\\lambda$ = 0.1</center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Comparison to Linear Regression\n",
    "Following figure shows comparison between cross entropy and mse losses.\n",
    "![alt text](linear_log_compare.png)\n",
    "<center>Figure 8: cross entropy and MSE loss comparison ($\\alpha$ = 0.005, $\\lambda$ = 0)</center>\n",
    "<br><br><br>",
    "\n",
    "From the figure, the value MSE losses is higher than the CE losses at each iteration. However, the difference between two methods rapidly decrease as more iteration computed, and both MSE and CE will gradually approach to zero. MSE has a much higher initial value than CE but drop quickly. Both MSE and CE are convex, CE triggers earlier than MSE. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 3: Batch Gradient Descent vs. SGD and Adam\n",
    "### 3.1 SGD\n",
    "Following is the code snippet of buildGraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildGraph(beta1, beta2, epsilon, loss=\"None\"):\n",
    "    W = tf.Variable(tf.truncated_normal([784, 1], stddev=0.5, dtype=tf.float32))\n",
    "    b = tf.Variable(tf.zeros(1))\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y = tf.placeholder(tf.float32, [None, 1])\n",
    "    lambda_ = tf.placeholder(tf.float32)\n",
    "    tf.set_random_seed(421)\n",
    "\n",
    "    if loss == \"MSE\":\n",
    "        y_hat = tf.matmul(x, W) + b\n",
    "        loss_t = 0.5 * tf.reduce_mean(tf.square(y - y_hat)) + lambda_ * tf.nn.l2_loss(W)\n",
    "    elif loss == \"CE\":\n",
    "        logits = (tf.matmul(x, W) + b)\n",
    "        loss_t = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits) + lambda_ * tf.nn.l2_loss(W)\n",
    "\n",
    "    adam_op = tf.train.AdamOptimizer(learning_rate=0.001, beta1=beta1, beta2=beta2, epsilon=epsilon).minimize(loss_t)\n",
    "    return x, y, W, b, lambda_, loss_t, adam_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implementing Stochastic Gradient Descent\n",
    "Following is the code snippet of SGD implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatch(minibatch_size, trainingData, trainingTarget, beta1, beta2, epsilon, plot_loss, lossType):\n",
    "    N, _ = trainingData.shape\n",
    "    x, y, W, b, lambda_, loss_t, adam_op = buildGraph(beta1, beta2, epsilon, loss=lossType)\n",
    "\n",
    "    n_epochs = 700\n",
    "    iterations = N // minibatch_size\n",
    "\n",
    "    # \"minibatch\" training\n",
    "    acc_train = np.zeros(n_epochs)\n",
    "    loss_train = np.zeros(n_epochs)\n",
    "    acc_valid = np.zeros(n_epochs)\n",
    "    loss_valid = np.zeros(n_epochs)\n",
    "    acc_test = np.zeros(n_epochs)\n",
    "    loss_test = np.zeros(n_epochs)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        #shuffle\n",
    "        s = np.arange(N)\n",
    "        np.random.shuffle(s)\n",
    "        trainingData = trainingData[s]\n",
    "        trainingTarget = trainingTarget[s]\n",
    "\n",
    "        #iterating mini batch\n",
    "        for j in range(iterations):\n",
    "            batch_data = trainingData[j*minibatch_size:(j+1)*minibatch_size, :]\n",
    "            batch_target = trainingTarget[j*minibatch_size:(j+1)*minibatch_size, :]\n",
    "            _, train_W, train_b = sess.run([adam_op, W, b], feed_dict={x: batch_data, y: batch_target, lambda_: 0})\n",
    "\n",
    "        #calc accuracy and loss for each epoch\n",
    "        acc_train[i] = np.sum((np.dot(trainingData, train_W) + train_b >= 0.5) == trainingTarget) / trainingTarget.shape[0]\n",
    "        loss_train[i] = sess.run(loss_t, feed_dict={x: trainingData, y: trainingTarget, lambda_: 0})\n",
    "        #valid and test\n",
    "        acc_valid[i] = np.sum((np.dot(validData, train_W) + train_b >= 0.5) == validTarget) / validTarget.shape[0]\n",
    "        loss_valid[i] = sess.run(loss_t, feed_dict={x: validData, y: validTarget, lambda_: 0})\n",
    "        acc_test[i] = np.sum((np.dot(testData, train_W) + train_b >= 0.5) == testTarget) / testTarget.shape[0]\n",
    "        loss_test[i] = sess.run(loss_t, feed_dict={x: testData, y: testTarget, lambda_: 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follosing figure shows a general case of minibatch size 500.\n",
    "![alt text](SGD_MSE_500.png)\n",
    "<center>Figure 9: SGD with a minibatch size of 500 (MSE, $\\lambda$ = 0)</center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Batch Size Investigation\n",
    "Following figures show the effects on minibatch size changeing.\n",
    "![alt text](SGD_MSE_100.png)\n",
    "<center>Figure 10: SGD with a minibatch size of 100 (MSE, $\\lambda$ = 0)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_MSE_700.png)\n",
    "<center>Figure 11: SGD with a minibatch size of 700 (MSE, $\\lambda$ = 0)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_MSE_1750.png)\n",
    "<center>Figure 12: SGD with a minibatch size of 1750 (MSE, $\\lambda$ = 0)</center>\n",
    "<br><br><br>",
    "\n",
    "The above cases indicate that accuracy decreses with increasing batch sizes. With larger batch size, the training accuracy becomes less convex, the validation and test accuracy becomes more flunctuated and overfitted. The larger batch size will cause more cycles of computation through each iteration, and it tends to converge to sharper minimals which cause the poorer generalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Hyperparameter Investigation\n",
    "Following figures show the effects on hyperparmeters.\n",
    "![alt text](SGD_MSE_BETA1.png)\n",
    "<center>Figure 13: SGD with $\\beta_{1}$ = 0.95, 0.99 (MSE, minibatch size 500)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_MSE_BETA2.png)\n",
    "<center>Figure 14: SGD with $\\beta_{2}$ = 0.99, 0.9999 (MSE, minibatch size 500)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_MSE_EPI.png)\n",
    "<center>Figure 15: SGD with $\\epsilon$ = 1e-9, 1e-4 (MSE, minibatch size 500)</center>\n",
    "<br><br><br>",
    "\n",
    "(a) Higher $\\beta_{1}$ results in a poorer performance of training accuracy, it's more flunctuated at the begining of the process but tends to process in a flat ratio.\n",
    "(b) higher $\\beta_{2}$ results in a poorer perfotmance of training accuracy, all three accuracies are relatively less convex.\n",
    "(c) Higher $\\epsilon$ results in a better performance of training accuracy, the validation and test accuracy are more fitted. \n"   
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Cross Entropy Loss Investigation\n",
    "Following figures show the losses calculated using cross entropy. The general demonstration uses minibatch size of 500. And the other hyperparameters investigation can be compared with those in Section 3.4.\n",
    "![alt text](SGD_CE_500.png)\n",
    "<center>Figure 16: SGD with a minibatch size of 500 (CE, $\\lambda$ = 0)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_CE_BETA1.png)\n",
    "<center>Figure 17: SGD with $\\beta_{1}$ = 0.95, 0.99 (CE, minibatch size 500)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_CE_BETA2.png)\n",
    "<center>Figure 18: SGD with $\\beta_{2}$ = 0.99, 0.9999 (CE, minibatch size 500)</center>\n",
    "<br><br><br>\n",
    "![alt text](SGD_CE_EPI.png)\n",
    "<center>Figure 19: SGD with $\\epsilon$ = 1e-9, 1e-4 (CE, minibatch size 500)</center>\n",
    "<br><br><br>",
    "\n",
    "Generally, cross entropy model has a better performance than SGD with Adam since the final accuracy which is generated by cross entropy model is approach to 1.0 in the cases that we computed. Using the cross entroy model, the accuracy increase dramatically at the first 100 iterations, the validation and test accuracy are almost well fitted at this period, while the SGD algorithm is more volatile at the beginning and then increase gradually in a flat ratio with less convex, the validation and test accruacy are more volatile and overfitted with this method. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Comparison against Batch GD\n",
    "Comparison between CE batch gradient descent and SGD:\n",
    "![alt](log_loss_acc.png)\n",
    "<center>Figure 7: losses and accuracy on all data (CE)</center>\n",
    "<br><br><br>\n",
    "![alt](SGD_CE_500.png)\n",
    "<center>Figure 16: SGD with a minibatch size of 500 (CE)</center>",
     "\n",
    "Generally, these two methods show similiar tends in performance and losses but SGD is more convex than Batch GD. For the accuracy, both SGD with Adam and Batch GD increase dramatically first and slowly approach to 1, the validation and test accuracy are almost fitting. In terms of losses, the results generated with the SGD algorithm is more convex than Batch GD as well, it decrease quickly to a small value and gradually approach to zero while the Batch GD performs like a concave curve. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
